# ç›®æ ‡æ£€æµ‹

[YOLOv5 Documentation](https://docs.ultralytics.com/tutorials/train-custom-datasets/)

## æ•°æ®é›†

çŽ¯å¢ƒï¼š
`Linux 5.4.72-microsoft-standard-WSL2 #1 SMP Wed Oct 28 23:40:43 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux`
`conda 4.11.0`
`Python 3.9.12`
`https://github.com/ultralytics/yolov5`

æ•°æ®é›†é‡‡ç”¨VOC2012ï¼Œä½†æ˜¯ultralyticsçš„YOLOç³»åˆ—é‡‡ç”¨çš„æ˜¯COCOæ•°æ®é›†çš„æ ‡æ³¨æ–¹å¼ï¼Œä¸èƒ½ç›´æŽ¥è®­ç»ƒã€‚

### æ•°æ®æ ‡æ³¨æ ¼å¼è½¬æ¢

> [å°†PASCAL VOCæ•°æ®é›†æ ¼å¼è½¬æ¢ä¸ºYOLOV5 æ‰€éœ€çš„æ ¼å¼](https://blog.csdn.net/qq_42978939/article/details/114777276)

å°†VOCæ ¼å¼çš„XMLæ–‡ä»¶è½¬æ¢ä¸ºCOCOçš„txtæ–‡ä»¶ï¼š

``` python
import os
import xml.etree.ElementTree as ET

classes = ["aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow", "diningtable", "dog",
           "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"]

# å°†x1, y1, x2, y2è½¬æ¢æˆyolov5æ‰€éœ€è¦çš„x, y, w, hæ ¼å¼
def xyxy2xywh(size, box):
    dw = 1. / size[0]
    dh = 1. / size[1]
    x = (box[0] + box[2]) / 2 * dw
    y = (box[1] + box[3]) / 2 * dh
    w = (box[2] - box[0]) * dw
    h = (box[3] - box[1]) * dh
    return (x, y, w, h)         # è¿”å›žçš„éƒ½æ˜¯æ ‡å‡†åŒ–åŽçš„å€¼


def voc2yolo(path):
    # å¯ä»¥æ‰“å°çœ‹çœ‹è¯¥è·¯å¾„æ˜¯å¦æ­£ç¡®
    print(len(os.listdir(path)))
    # éåŽ†æ¯ä¸€ä¸ªxmlæ–‡ä»¶
    for file in os.listdir(path):
        # xmlæ–‡ä»¶çš„å®Œæ•´è·¯å¾„, æ³¨æ„ï¼šå› ä¸ºæ˜¯è·¯å¾„æ‰€ä»¥è¦ç¡®ä¿å‡†ç¡®ï¼Œæˆ‘æ˜¯ç›´æŽ¥ä½¿ç”¨äº†å­—ç¬¦ä¸²æ‹¼æŽ¥, ä¸ºäº†ä¿é™©å¯ä»¥ç”¨os.path.join(path, file)
        label_file = path + file
        # æœ€ç»ˆè¦æ”¹æˆçš„txtæ ¼å¼æ–‡ä»¶,è¿™é‡Œæˆ‘æ˜¯æ”¾åœ¨voc2007/labels/ä¸‹é¢
        # æ³¨æ„: labelsæ–‡ä»¶å¤¹å¿…é¡»å­˜åœ¨ï¼Œæ²¡æœ‰å°±å…ˆåˆ›å»ºï¼Œä¸ç„¶ä¼šæŠ¥é”™
        out_file = open(path.replace('Annotations', 'labels') + file.replace('xml', 'txt'), 'w')
        # print(label_file)

        # å¼€å§‹è§£æžxmlæ–‡ä»¶
        tree = ET.parse(label_file)
        root = tree.getroot()
        size = root.find('size')            # å›¾ç‰‡çš„shapeå€¼
        w = int(size.find('width').text)
        h = int(size.find('height').text)

        for obj in root.iter('object'):
            difficult = obj.find('difficult').text
            cls = obj.find('name').text
            if cls not in classes or int(difficult) == 1:
                continue
            # å°†åç§°è½¬æ¢ä¸ºidä¸‹æ ‡
            cls_id = classes.index(cls)
            # èŽ·å–æ•´ä¸ªbounding boxæ¡†
            bndbox = obj.find('bndbox')
            # xmlç»™å‡ºçš„æ˜¯x1, y1, x2, y2
            box = [float(bndbox.find('xmin').text), float(bndbox.find('ymin').text), float(bndbox.find('xmax').text),
                float(bndbox.find('ymax').text)]

            # å°†x1, y1, x2, y2è½¬æ¢æˆyolov5æ‰€éœ€è¦çš„x_center, y_center, w, hæ ¼å¼
            bbox = xyxy2xywh((w, h), box)
            # å†™å…¥ç›®æ ‡æ–‡ä»¶ä¸­ï¼Œæ ¼å¼ä¸º id x y w h
            out_file.write(str(cls_id) + " " + " ".join(str(x) for x in bbox) + '\n')

if __name__ == '__main__':
 # è¿™é‡Œè¦æ”¹æˆè‡ªå·±æ•°æ®é›†è·¯å¾„çš„æ ¼å¼
    path = '/home/u/data/VOCdevkit/VOC2012/Annotations/'
    voc2yolo(path)


```

### è®­ç»ƒå›¾ç‰‡ã€æ ‡æ³¨ç›®å½•å˜æ¢

VOCæ•°æ®é›†ç›®å½•æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­å›¾ç‰‡æ–‡ä»¶å…¨æ”¾åœ¨JPEGImagesç›®å½•ä¸‹ï¼Œæ²¡æœ‰é’ˆå¯¹train/valåˆ†åˆ«å­˜æ”¾ã€‚

```
VOC2012
    â”œâ”€â”€ Annotations  è¿›è¡Œdetection ä»»åŠ¡æ—¶çš„ æ ‡ç­¾æ–‡ä»¶ï¼Œxmlæ–‡ä»¶å½¢å¼
    â”œâ”€â”€ ImageSets  å­˜æ”¾æ•°æ®é›†çš„åˆ†å‰²æ–‡ä»¶ï¼Œæ¯”å¦‚trainï¼Œvalï¼Œtest
    â”œâ”€â”€ JPEGImages  å­˜æ”¾ .jpgæ ¼å¼çš„å›¾ç‰‡æ–‡ä»¶
    â”œâ”€â”€ SegmentationClass  å­˜æ”¾ æŒ‰ç…§class åˆ†å‰²çš„å›¾ç‰‡
    â””â”€â”€ SegmentationObject  å­˜æ”¾ æŒ‰ç…§ object åˆ†å‰²çš„å›¾ç‰‡
```

è€ŒYOLOé‡Œé»˜è®¤çš„COCOæ•°æ®é›†ç›®å½•æ ¼å¼å¦‚ä¸‹ï¼š

```
COCO
    â”œâ”€â”€ images  # å­˜æ”¾å›¾ç‰‡æ–‡ä»¶
    â”‚     â”œâ”€â”€ train
    â”‚     â”œâ”€â”€ trainval
    â”‚     â””â”€â”€ val
    â””â”€â”€ labels  # å­˜æ”¾æ ‡æ³¨æ–‡ä»¶
          â”œâ”€â”€ train
          â”œâ”€â”€ trainval
          â””â”€â”€ val

```

**æ³¨æ„ï¼šè¿è¡Œä¸‹æ–¹è„šæœ¬å‰éœ€è¦äº‹å…ˆåˆ›å»ºå¥½ç›¸åº”COCOåŠå…¶å­ç›®å½•**
**å¯¹åº”çš„è·¯å¾„ä¹Ÿè¦æ ¹æ®è‡ªå·±è®¾å¤‡è¿›è¡Œé€‚å½“ä¿®æ”¹**

- å˜æ¢å›¾ç‰‡ç›®å½•

> [VOC/COCO/YOLOæ•°æ®æ€»ç»“åŠè½¬æ¢](https://zhuanlan.zhihu.com/p/160103709)

```python
import xml.etree.ElementTree as ET
import os
import json
import shutil

def check_make_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def make_coco_dir(root):
    check_make_dir(os.path.join(root, 'Annotations'))
    check_make_dir(os.path.join(root, 'Annotations_xml'))
    check_make_dir(os.path.join(root, 'Annotations_xml'))
    check_make_dir(os.path.join(root, 'Annotations_xml'))
    check_make_dir(os.path.join(root, 'Annotations_xml'))
    check_make_dir(os.path.join(root, 'Annotations_xml'))
    check_make_dir(os.path.join(root, 'xml_list'))
    check_make_dir(os.path.join(root, 'images'))
    # check_make_dir(os.path.join(root, 'VOC2007/Annotations'))

if __name__ == '__main__':
    # ROOT = "/PATH/TO/VOC2007/"
    ROOT = "/home/u/data/VOCdevkit/VOC2012"
    img_dir = os.path.join(ROOT, "JPEGImages")
    xml_dir = os.path.join(ROOT, "Annotations")
    division_dir = os.path.join(ROOT, "ImageSets/Main")
    #
    # target_dir = "/PATH/TO/COCO"
    target_dir = "/home/u/data/VOCdevkit/COCO"
    make_coco_dir(target_dir)
    # you should make those directions below in target_dir/images/
    # file_list = ['test.txt', 'trainval.txt', 'train.txt', 'val.txt']
    file_list = ['trainval.txt', 'train.txt', 'val.txt']
    # 'train.txt', 'val.txt'
    for file_name in file_list:
        path = os.path.join(division_dir, file_name)
    #     divide xml into different set: train, test, val
        file = open(path)
        for line in file.readlines():
            line = line[:-1]
            img_path = os.path.join(img_dir, line+'.jpg')
            target_path = os.path.join(target_dir, 'images', file_name[:-4], line+'.jpg')
            # shutil.copy(img_path, target_path)
            shutil.copy(os.path.join(xml_dir, line+'.xml'), os.path.join(target_dir, 'Annotations_xml', 'xml_' + file_name[:-4], line+'.xml'))
            # shutil.copy(os.path.join(xml_dir, line + '.xml'), os.path.join(target_dir, 'Annotations_xml', line+'.xml'))
```

- å˜æ¢æ ‡æ³¨ç›®å½•

``` python
import shutil
import os

if __name__ == '__main__':
    ROOT = "/home/u/data/VOCdevkit/VOC2012"
    img_dir = os.path.join(ROOT, "JPEGImages")
    xml_dir = os.path.join(ROOT, "Annotations")
    label_dir = os.path.join(ROOT, "labels")
    division_dir = os.path.join(ROOT, "ImageSets/Main")
    #
    target_dir = "/home/u/data/VOCdevkit/COCO"
    # you should make those directions below in target_dir/images/
    # file_list = ['test.txt', 'trainval.txt', 'train.txt', 'val.txt']
    file_list = ['trainval.txt', 'train.txt', 'val.txt']
    for file_name in file_list:
        path = os.path.join(division_dir, file_name)
        file = open(path)
        for line in file.readlines():
            line = line[:-1]
            label_path = os.path.join(label_dir, line+'.txt')
            label_target_path = os.path.join(target_dir, 'labels', file_name[:-4], line+'.txt')
            shutil.copy(label_path, label_target_path)
```

### åˆ›å»ºæ­¤æ•°æ®é›†å¯¹åº”çš„dataset.yaml

``` yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/VOC  # dataset root dir
train: images/train  # train images (relative to 'path') 128 images
val: images/val  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
nc: 20  # number of classes
names: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
        'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']  # class names
```

## æ¨¡åž‹è½¬æ¢

[android-demo-app/ObjectDetection](https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection)

é€šè¿‡ultralytics/yolov5å†…çš„`export.py`è„šæœ¬å®Œæˆã€‚

ä¸è¿‡å¯èƒ½æ˜¯å› ä¸ºultralytics/yolov5çš„ç‰ˆæœ¬æ›´æ–°ï¼Œç›®å‰çš„`export.py`å·²ç»ä¸Ž`android-demo-app/ObjectDetection`ä»‹ç»ä¸­çš„`export.py`è„šæœ¬ä¸åŒäº†ï¼Œæ²¡æ³•æŒ‰ç…§READMEä¿®æ”¹äº†ã€‚

ä¸è¿‡æˆ‘åœ¨githubçš„[issues 5533](https://github.com/ultralytics/yolov5/issues/5533)ä¸­æ‰¾åˆ°äº†*å¯èƒ½æ˜¯*æ—§ç‰ˆ`export.py`çš„ç›¸å…³ä»£ç ï¼š

``` python
# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license
"""
Export a PyTorch model to TorchScript, ONNX, CoreML formats

Usage:
    $ python path/to/export.py --weights yolov5s.pt --img 640 --batch 1
"""

import argparse
import sys
import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.mobile_optimizer import optimize_for_mobile

FILE = Path(__file__).absolute()
sys.path.append(FILE.parents[0].as_posix())  # add yolov5/ to path
sys.path.insert(0, './yolov5')
from models.common import Conv
from models.yolo import Detect
from models.experimental import attempt_load
from utils.activations import Hardswish, SiLU
from utils.general import colorstr, check_img_size, check_requirements, file_size, set_logging
from utils.torch_utils import select_device


def export_torchscript(model, img, file, optimize):
    # TorchScript model export
    prefix = colorstr('TorchScript:')
    try:
        print(f'\n{prefix} starting export with torch {torch.__version__}...')
        f = file.with_suffix('.torchscript.pt')
        ts = torch.jit.trace(model, img, strict=False)
        (optimize_for_mobile(ts) if optimize else ts).save(f)
        print(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
        return ts
    except Exception as e:
        print(f'{prefix} export failure: {e}')


def export_onnx(model, img, file, opset, train, dynamic, simplify):
    # ONNX model export
    prefix = colorstr('ONNX:')
    try:
        check_requirements(('onnx', 'onnx-simplifier'))
        import onnx

        print(f'\n{prefix} starting export with onnx {onnx.__version__}...')
        f = file.with_suffix('.onnx')
        torch.onnx.export(model, img, f, verbose=False, opset_version=opset,
                          training=torch.onnx.TrainingMode.TRAINING if train else torch.onnx.TrainingMode.EVAL,
                          do_constant_folding=not train,
                          input_names=['images'],
                          output_names=['output'],
                          dynamic_axes={'images': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1,3,640,640)
                                        'output': {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)
                                        } if dynamic else None)

        # Checks
        model_onnx = onnx.load(f)  # load onnx model
        onnx.checker.check_model(model_onnx)  # check onnx model
        # print(onnx.helper.printable_graph(model_onnx.graph))  # print

        # Simplify
        if simplify:
            try:
                import onnxsim

                print(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')
                model_onnx, check = onnxsim.simplify(
                    model_onnx,
                    dynamic_input_shape=dynamic,
                    input_shapes={'images': list(img.shape)} if dynamic else None)
                assert check, 'assert check failed'
                onnx.save(model_onnx, f)
            except Exception as e:
                print(f'{prefix} simplifier failure: {e}')
        print(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
        print(f"{prefix} run --dynamic ONNX model inference with: 'python detect.py --weights {f}'")
    except Exception as e:
        print(f'{prefix} export failure: {e}')


def export_coreml(model, img, file):
    # CoreML model export
    prefix = colorstr('CoreML:')
    try:
        check_requirements(('coremltools',))
        import coremltools as ct

        print(f'\n{prefix} starting export with coremltools {ct.__version__}...')
        f = file.with_suffix('.mlmodel')
        model.train()  # CoreML exports should be placed in model.train() mode
        ts = torch.jit.trace(model, img, strict=False)  # TorchScript model
        model = ct.convert(ts, inputs=[ct.ImageType('image', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])])
        model.save(f)
        print(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
    except Exception as e:
        print(f'\n{prefix} export failure: {e}')


def run(weights='model.pt',  # weights path
        img_size=(640, 640),  # image (height, width)
        batch_size=1,  # batch size
        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        include=('torchscript', 'onnx', 'coreml'),  # include formats
        half=False,  # FP16 half-precision export
        inplace=True,  # set YOLOv5 Detect() inplace=True
        train=False,  # model.train() mode
        optimize=False,  # TorchScript: optimize for mobile
        dynamic=False,  # ONNX: dynamic axes
        simplify=False,  # ONNX: simplify model
        opset=10,  # ONNX: opset version
        ):
    t = time.time()
    include = [x.lower() for x in include]
    img_size *= 2 if len(img_size) == 1 else 1  # expand
    file = Path(weights)

    # Load PyTorch model
    device = select_device(device)
    assert not (device.type == 'cpu' and half), '--half only compatible with GPU export, i.e. use --device 0'
    model = attempt_load(weights, map_location=device)  # load FP32 model
    names = model.names

    # Input
    gs = int(max(model.stride))  # grid size (max stride)
    img_size = [check_img_size(x, gs) for x in img_size]  # verify img_size are gs-multiples
    img = torch.zeros(batch_size, 3, *img_size).to(device)  # image size(1,3,320,192) iDetection

    # Update model
    if half:
        img, model = img.half(), model.half()  # to FP16
    model.train() if train else model.eval()  # training mode = no Detect() layer grid construction
    for k, m in model.named_modules():
        if isinstance(m, Conv):  # assign export-friendly activations
            if isinstance(m.act, nn.Hardswish):
                m.act = Hardswish()
            elif isinstance(m.act, nn.SiLU):
                m.act = SiLU()
        elif isinstance(m, Detect):
            m.inplace = inplace
            m.onnx_dynamic = dynamic
            # m.forward = m.forward_export  # assign forward (optional)

    for _ in range(2):
        y = model(img)  # dry runs
    print(f"\n{colorstr('PyTorch:')} starting from {weights} ({file_size(weights):.1f} MB)")

    # Exports
    if 'torchscript' in include:
        export_torchscript(model, img, file, optimize)
    if 'onnx' in include:
        export_onnx(model, img, file, opset, train, dynamic, simplify)
    if 'coreml' in include:
        export_coreml(model, img, file)

    # Finish
    print(f'\nExport complete ({time.time() - t:.2f}s)'
          f"\nResults saved to {colorstr('bold', file.parent.resolve())}"
          f'\nVisualize with https://netron.app')


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default='model.pt', help='weights path')
    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='image (height, width)')
    parser.add_argument('--batch-size', type=int, default=1, help='batch size')
    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--include', nargs='+', default=['torchscript', 'onnx', 'coreml'], help='include formats')
    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')
    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')
    parser.add_argument('--train', action='store_true', help='model.train() mode')
    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')
    parser.add_argument('--dynamic', action='store_true', help='ONNX: dynamic axes')
    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')
    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')
    opt = parser.parse_args()
    return opt


def main(opt):
    set_logging()
    print(colorstr('export: ') + ', '.join(f'{k}={v}' for k, v in vars(opt).items()))
    run(**vars(opt))


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
```

è¿™æ ·ï¼Œå°±åªè¦æ ¹æ®[android-demo-app/ObjectDetection](https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection)çš„READMEä¸­å†™çš„ï¼š

> Edit export.py to make the following two changes:
>
> After f = file.with_suffix('.torchscript.pt'), add a line fl = file.with_suffix('.torchscript.ptl')
>
> After (optimize_for_mobile(ts) if optimize else ts).save(f), add (optimize_for_mobile(ts) if optimize else ts)._save_for_lite_interpreter(str(fl))

å†å°†ç¬¬116è¡Œä»£ç `optimize=False,  # TorchScript: optimize for mobile`æ”¹ä¸º`optimize=True,  # TorchScript: optimize for mobile`

å°±èƒ½å¤Ÿæ­£å¸¸åœ°å¯¼å‡ºç§»åŠ¨è®¾å¤‡çš„è¿›ä¸€æ­¥ä¼˜åŒ–(Further optimization for mobile devices)çš„æ¨¡åž‹äº†ã€‚

![export.png](./images/export.png)

æ‰“å¼€`/home/u/TEST/Proj/yolov5/runs/train/exp5/weights`ç›®å½•ï¼Œå°±èƒ½æ‰¾åˆ°æˆ‘ä»¬éœ€è¦çš„`best.torchscript.ptl`æ–‡ä»¶äº†ã€‚

## ä¿®æ”¹Androidé¡¹ç›®ä»¥é€‚é…æ–°æ¨¡åž‹

1. æ›´æ”¹ptlæ¨¡åž‹ã€ç±»åž‹æ–‡ä»¶

> change lines:
> `mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), "yolov5s.torchscript.ptl"));`
> `BufferedReader br = new BufferedReader(new InputStreamReader(getAssets().open("classes.txt")));`
> to:
> `mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), "best.torchscript.ptl"));`
> `BufferedReader br = new BufferedReader(new InputStreamReader(getAssets().open("aicook.txt")));`

è¿™é‡Œï¼Œæˆ‘ä»¬æ”¹ä¸º`best.torchscript.ptl`å’Œ`VOC.txt`ã€‚

2. æ›´æ”¹é¢„å¤„ç†ä¸­çš„ç±»åž‹ä¸ªæ•°

> Then in `PrePostProcessor.java`, change line `private static int mOutputColumn = 85;` to `private static int mOutputColumn = 35;`.

è¿™é‡Œï¼Œæˆ‘ä»¬æ”¹ä¸º`private static int mOutputColumn = 25; // left, top, right, bottom, score and 20 class probability`ï¼ŒVOCæ•°æ®åº“20ä¸ªç±»åž‹+5

3. æ›´æ”¹å®žæ—¶æ£€æµ‹ä¸­çš„æ¨¡åž‹æ–‡ä»¶

> In order to do live object detection with the new custom model, just open `ObjectDetectionActivity.java` and replace `yolov5s.torchscript.ptl` in `mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), "yolov5s.torchscript.ptl"));` with `best.torchscript.ptl`.

|2022å¹´4æœˆ14æ—¥|
|----:|

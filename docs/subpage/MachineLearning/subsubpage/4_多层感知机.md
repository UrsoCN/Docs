# 4 多层感知机

## 4.2 多层感知机的从零开始实现

理论部分可以参考其他书目，如：

> 《统计学习方法》 李航  
> 《最优化：建模、算法与理论》 刘浩洋、户将、李勇锋、文再文  
> 《机器学习》 周志华

> 4.1.1.3. 从线性到非线性  
注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？ 你可能会惊讶地发现：在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。

要注意激活函数σ(⋅)在多层网络(多层感知机)中的作用，而这样的激活函数σ需要是非线性函数。最简单的ReLU(Rectified linear unit)函数就是在线性函数上修改的来的，且兼顾了运算复杂度。
> 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现的更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后将详细介绍）。

在这一节的中文网页中有几处错误：[4.2. 多层感知机的从零开始实现](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/mlp-scratch.html)，具体表现为训练画出的图中看不到train loss曲线

1. d2l包中封装的``train_epoch_ch3``函数出现错误(在[3.6. softmax回归的从零开始实现](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)中编写)。在GitHub中文版的[d2l-zh/d2l/torch.py](https://github.com/d2l-ai/d2l-zh/blob/master/d2l/torch.py)的函数能正确运行。
	- 可以直接修改``\home\u\miniconda3\lib\python3.9\site-packages\d2l\torch.py``中的``train_epoch_ch3``函数为

	``` python
	def train_epoch_ch3(net, train_iter, loss, updater):
    """训练模型一个迭代周期（定义见第3章）
    Defined in :numref:`sec_softmax_scratch`"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
    ```

    <font color=#FF000>具体就是if语句中第一个反向传播错写为了</font> `l.sum().backward()`

2. 损失函数出错，GitHub中文版[多层感知机的从零开始实现](https://github.com/d2l-ai/d2l-zh/blob/master/chapter_multilayer-perceptrons/mlp-scratch.md)中有额外的reduction参数 `loss = nn.CrossEntropyLoss(reduction='none')`

正常来讲如果是用`pip install d2l-zh`就不会出问题的，正在纠结是不是要重新搞QAQ

## 4.3 多层感知机的简洁实现

损失函数同样出错了。

``` python
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
```

在搭建好网络模型之后，一个重要的步骤就是对网络模型中的权值进行初始化。适当的权值初始化可以加快模型的收敛，而不恰当的权值初始化可能引发梯度消失或者梯度爆炸，最终导致模型无法收敛。

在这里，`std=0.01`改为0.1能更快地收敛。

多加一层，能够稍微提高一些准确率，但还不如提高`num_epochs`。  
在激活函数中，`nn.ReLU()`能比`nn.Sigmoid()`更快的收敛，但使用`nn.Sigmoid()`时，`train loss`与`test loss`贴合的要好很多。

``` python
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 20),
                    nn.ReLU(),
                    nn.Linear(20, 10))
```

## 4.4 模型选择、欠拟合和过拟合

估计模型容量  
模型容量主要由**参数的个数**与**参数值的选择范围**影响

